I1129 23:26:11.757902   685 caffe.cpp:102] Use GPU with device ID 0
I1129 23:26:11.759999   685 caffe.cpp:110] Starting Optimization
I1129 23:26:11.760103   685 solver.cpp:32] Initializing solver from parameters: 
test_iter: 100
test_interval: 100
base_lr: 0.01
display: 50
max_iter: 700
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 200
snapshot_prefix: "snapshot/sicnn_train10k_mnist-sc_split6"
solver_mode: GPU
net: "protos/sicnn_train10k_train_test_mnist-sc_split6.prototxt"
epoch_size: 78
save_max: true
I1129 23:26:11.760138   685 solver.cpp:71] Creating training net from net file: protos/sicnn_train10k_train_test_mnist-sc_split6.prototxt
I1129 23:26:11.760535   685 net.cpp:281] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1129 23:26:11.760568   685 net.cpp:281] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1129 23:26:11.760789   685 net.cpp:41] Initializing net from parameters: 
name: "MNIST-sicnn-Table-1-split-6"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: HDF5_DATA
  hdf5_data_param {
    source: "../../data/mnist/table1/10k_split6_test.txt"
    batch_size: 128
  }
  include {
    phase: TRAIN
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: TICONV
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 36
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
  transformations {
  }
  transformations {
    scale: 0.63
  }
  transformations {
    scale: 0.7937
  }
  transformations {
    scale: 1.2599
  }
  transformations {
    scale: 1.5874
  }
  transformations {
    scale: 2
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: TICONV
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 64
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
  transformations {
  }
  transformations {
    scale: 0.63
  }
  transformations {
    scale: 0.7937
  }
  transformations {
    scale: 1.2599
  }
  transformations {
    scale: 1.5874
  }
  transformations {
    scale: 2
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layers {
  bottom: "pool2"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 150
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1129 23:26:11.760895   685 net.cpp:69] Creating Layer mnist
I1129 23:26:11.760910   685 net.cpp:362] mnist -> data
I1129 23:26:11.760931   685 net.cpp:362] mnist -> label
I1129 23:26:11.760946   685 net.cpp:98] Setting up mnist
I1129 23:26:11.760957   685 hdf5_data_layer.cpp:61] Loading filename from ../../data/mnist/table1/10k_split6_test.txt
I1129 23:26:11.761165   685 hdf5_data_layer.cpp:73] Number of files: 1
I1129 23:26:11.761185   685 hdf5_data_layer.cpp:76] Loading HDF5 file/root/si-convnet/data/mnist/table1/mnist-sc-uniform-10ksplit-set-6.h5
I1129 23:26:11.792964   685 hdf5_data_layer.cpp:86] output data size: 128,1,28,28
I1129 23:26:11.793095   685 net.cpp:105] Top shape: 128 1 28 28 (100352)
I1129 23:26:11.793110   685 net.cpp:105] Top shape: 128 1 1 1 (128)
I1129 23:26:11.793138   685 net.cpp:69] Creating Layer conv1
I1129 23:26:11.793148   685 net.cpp:400] conv1 <- data
I1129 23:26:11.793164   685 net.cpp:362] conv1 -> conv1
I1129 23:26:11.793184   685 net.cpp:98] Setting up conv1
I1129 23:26:11.793215   685 ticonv_layer.cpp:27] TIConvolution layer using 6 transformations conv1 using interpolation: 1
I1129 23:26:11.793228   685 ticonv_layer.cpp:33]  T0 :  sc: 1, rot: 0
I1129 23:26:11.793300   685 ticonv_layer.cpp:33]  T1 :  sc: 0.63, rot: 0
I1129 23:26:11.793316   685 ticonv_layer.cpp:33]  T2 :  sc: 0.7937, rot: 0
I1129 23:26:11.793329   685 ticonv_layer.cpp:33]  T3 :  sc: 1.2599, rot: 0
I1129 23:26:11.793341   685 ticonv_layer.cpp:33]  T4 :  sc: 1.5874, rot: 0
I1129 23:26:11.793354   685 ticonv_layer.cpp:33]  T5 :  sc: 2, rot: 0
I1129 23:26:11.793366   685 ticonv_layer.cpp:37]   Creating Upsampling Layer in conv1
I1129 23:26:11.807054   685 ticonv_layer.cpp:51]   Top shape: 1 28 28
I1129 23:26:11.807080   685 ticonv_layer.cpp:51]   Top shape: 1 17 17
I1129 23:26:11.807090   685 ticonv_layer.cpp:51]   Top shape: 1 22 22
I1129 23:26:11.807098   685 ticonv_layer.cpp:51]   Top shape: 1 35 35
I1129 23:26:11.807106   685 ticonv_layer.cpp:51]   Top shape: 1 44 44
I1129 23:26:11.807114   685 ticonv_layer.cpp:51]   Top shape: 1 56 56
I1129 23:26:11.807122   685 ticonv_layer.cpp:58]   Creating TiedConv Layer in conv1
I1129 23:26:11.807289   685 ticonv_layer.cpp:70]   Top shape: 36 22 22
I1129 23:26:11.807307   685 ticonv_layer.cpp:70]   Top shape: 36 11 11
I1129 23:26:11.807315   685 ticonv_layer.cpp:70]   Top shape: 36 16 16
I1129 23:26:11.807323   685 ticonv_layer.cpp:70]   Top shape: 36 29 29
I1129 23:26:11.807332   685 ticonv_layer.cpp:70]   Top shape: 36 38 38
I1129 23:26:11.807340   685 ticonv_layer.cpp:70]   Top shape: 36 50 50
I1129 23:26:11.807349   685 ticonv_layer.cpp:82]   Creating DownPooling Layer in conv1
I1129 23:26:11.807847   685 net.cpp:105] Top shape: 128 36 22 22 (2230272)
I1129 23:26:11.807889   685 net.cpp:69] Creating Layer relu1
I1129 23:26:11.807904   685 net.cpp:400] relu1 <- conv1
I1129 23:26:11.807917   685 net.cpp:351] relu1 -> conv1 (in-place)
I1129 23:26:11.807930   685 net.cpp:98] Setting up relu1
I1129 23:26:11.807940   685 net.cpp:105] Top shape: 128 36 22 22 (2230272)
I1129 23:26:11.807953   685 net.cpp:69] Creating Layer pool1
I1129 23:26:11.807961   685 net.cpp:400] pool1 <- conv1
I1129 23:26:11.807974   685 net.cpp:362] pool1 -> pool1
I1129 23:26:11.807987   685 net.cpp:98] Setting up pool1
I1129 23:26:11.808001   685 net.cpp:105] Top shape: 128 36 11 11 (557568)
I1129 23:26:11.808017   685 net.cpp:69] Creating Layer conv2
I1129 23:26:11.808027   685 net.cpp:400] conv2 <- pool1
I1129 23:26:11.808039   685 net.cpp:362] conv2 -> conv2
I1129 23:26:11.808055   685 net.cpp:98] Setting up conv2
I1129 23:26:11.808064   685 ticonv_layer.cpp:27] TIConvolution layer using 6 transformations conv2 using interpolation: 1
I1129 23:26:11.808073   685 ticonv_layer.cpp:33]  T0 :  sc: 1, rot: 0
I1129 23:26:11.808091   685 ticonv_layer.cpp:33]  T1 :  sc: 0.63, rot: 0
I1129 23:26:11.808109   685 ticonv_layer.cpp:33]  T2 :  sc: 0.7937, rot: 0
I1129 23:26:11.808122   685 ticonv_layer.cpp:33]  T3 :  sc: 1.2599, rot: 0
I1129 23:26:11.808135   685 ticonv_layer.cpp:33]  T4 :  sc: 1.5874, rot: 0
I1129 23:26:11.808145   685 ticonv_layer.cpp:33]  T5 :  sc: 2, rot: 0
I1129 23:26:11.808157   685 ticonv_layer.cpp:37]   Creating Upsampling Layer in conv2
I1129 23:26:11.808681   685 ticonv_layer.cpp:51]   Top shape: 36 11 11
I1129 23:26:11.808708   685 ticonv_layer.cpp:51]   Top shape: 36 6 6
I1129 23:26:11.808718   685 ticonv_layer.cpp:51]   Top shape: 36 8 8
I1129 23:26:11.808727   685 ticonv_layer.cpp:51]   Top shape: 36 13 13
I1129 23:26:11.808734   685 ticonv_layer.cpp:51]   Top shape: 36 17 17
I1129 23:26:11.808742   685 ticonv_layer.cpp:51]   Top shape: 36 22 22
I1129 23:26:11.808749   685 ticonv_layer.cpp:58]   Creating TiedConv Layer in conv2
I1129 23:26:11.812255   685 ticonv_layer.cpp:70]   Top shape: 64 7 7
I1129 23:26:11.812276   685 ticonv_layer.cpp:70]   Top shape: 64 2 2
I1129 23:26:11.812284   685 ticonv_layer.cpp:70]   Top shape: 64 4 4
I1129 23:26:11.812294   685 ticonv_layer.cpp:70]   Top shape: 64 9 9
I1129 23:26:11.812300   685 ticonv_layer.cpp:70]   Top shape: 64 13 13
I1129 23:26:11.812309   685 ticonv_layer.cpp:70]   Top shape: 64 18 18
I1129 23:26:11.812317   685 ticonv_layer.cpp:82]   Creating DownPooling Layer in conv2
I1129 23:26:11.812613   685 net.cpp:105] Top shape: 128 64 7 7 (401408)
I1129 23:26:11.812642   685 net.cpp:69] Creating Layer relu2
I1129 23:26:11.812654   685 net.cpp:400] relu2 <- conv2
I1129 23:26:11.812665   685 net.cpp:351] relu2 -> conv2 (in-place)
I1129 23:26:11.812677   685 net.cpp:98] Setting up relu2
I1129 23:26:11.812687   685 net.cpp:105] Top shape: 128 64 7 7 (401408)
I1129 23:26:11.812700   685 net.cpp:69] Creating Layer pool2
I1129 23:26:11.812708   685 net.cpp:400] pool2 <- conv2
I1129 23:26:11.812718   685 net.cpp:362] pool2 -> pool2
I1129 23:26:11.812731   685 net.cpp:98] Setting up pool2
I1129 23:26:11.812741   685 net.cpp:105] Top shape: 128 64 3 3 (73728)
I1129 23:26:11.812757   685 net.cpp:69] Creating Layer ip1
I1129 23:26:11.812764   685 net.cpp:400] ip1 <- pool2
I1129 23:26:11.812781   685 net.cpp:362] ip1 -> ip1
I1129 23:26:11.812798   685 net.cpp:98] Setting up ip1
I1129 23:26:11.817981   685 net.cpp:105] Top shape: 128 150 1 1 (19200)
I1129 23:26:11.818018   685 net.cpp:69] Creating Layer relu3
I1129 23:26:11.818032   685 net.cpp:400] relu3 <- ip1
I1129 23:26:11.818044   685 net.cpp:351] relu3 -> ip1 (in-place)
I1129 23:26:11.818061   685 net.cpp:98] Setting up relu3
I1129 23:26:11.818070   685 net.cpp:105] Top shape: 128 150 1 1 (19200)
I1129 23:26:11.818084   685 net.cpp:69] Creating Layer ip2
I1129 23:26:11.818092   685 net.cpp:400] ip2 <- ip1
I1129 23:26:11.818104   685 net.cpp:362] ip2 -> ip2
I1129 23:26:11.818116   685 net.cpp:98] Setting up ip2
I1129 23:26:11.818229   685 net.cpp:105] Top shape: 128 10 1 1 (1280)
I1129 23:26:11.818255   685 net.cpp:69] Creating Layer loss
I1129 23:26:11.818267   685 net.cpp:400] loss <- ip2
I1129 23:26:11.818279   685 net.cpp:400] loss <- label
I1129 23:26:11.818289   685 net.cpp:362] loss -> loss
I1129 23:26:11.818303   685 net.cpp:98] Setting up loss
I1129 23:26:11.818320   685 net.cpp:105] Top shape: 1 1 1 1 (1)
I1129 23:26:11.818329   685 net.cpp:112]     with loss weight 1
I1129 23:26:11.818353   685 net.cpp:175] loss needs backward computation.
I1129 23:26:11.818364   685 net.cpp:175] ip2 needs backward computation.
I1129 23:26:11.818378   685 net.cpp:175] relu3 needs backward computation.
I1129 23:26:11.818387   685 net.cpp:175] ip1 needs backward computation.
I1129 23:26:11.818395   685 net.cpp:175] pool2 needs backward computation.
I1129 23:26:11.818404   685 net.cpp:175] relu2 needs backward computation.
I1129 23:26:11.818413   685 net.cpp:175] conv2 needs backward computation.
I1129 23:26:11.818421   685 net.cpp:175] pool1 needs backward computation.
I1129 23:26:11.818429   685 net.cpp:175] relu1 needs backward computation.
I1129 23:26:11.818437   685 net.cpp:175] conv1 needs backward computation.
I1129 23:26:11.818446   685 net.cpp:177] mnist does not need backward computation.
I1129 23:26:11.818454   685 net.cpp:214] This network produces output loss
I1129 23:26:11.818473   685 net.cpp:473] Collecting Learning Rate and Weight Decay.
I1129 23:26:11.818488   685 net.cpp:225] Network initialization done.
I1129 23:26:11.818500   685 net.cpp:226] Memory required for data: 24155976
I1129 23:26:11.818907   685 solver.cpp:154] Creating test net (#0) specified by net file: protos/sicnn_train10k_train_test_mnist-sc_split6.prototxt
I1129 23:26:11.818959   685 net.cpp:281] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1129 23:26:11.819178   685 net.cpp:41] Initializing net from parameters: 
name: "MNIST-sicnn-Table-1-split-6"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: HDF5_DATA
  hdf5_data_param {
    source: "../../data/mnist/table1/10k_split6_train.txt"
    batch_size: 100
  }
  include {
    phase: TEST
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: TICONV
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 36
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
  transformations {
  }
  transformations {
    scale: 0.63
  }
  transformations {
    scale: 0.7937
  }
  transformations {
    scale: 1.2599
  }
  transformations {
    scale: 1.5874
  }
  transformations {
    scale: 2
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: TICONV
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 64
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
  transformations {
  }
  transformations {
    scale: 0.63
  }
  transformations {
    scale: 0.7937
  }
  transformations {
    scale: 1.2599
  }
  transformations {
    scale: 1.5874
  }
  transformations {
    scale: 2
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layers {
  bottom: "pool2"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 150
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  name: "accuracy"
  type: ACCURACY
  include {
    phase: TEST
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TEST
}
I1129 23:26:11.819352   685 net.cpp:69] Creating Layer mnist
I1129 23:26:11.819370   685 net.cpp:362] mnist -> data
I1129 23:26:11.819386   685 net.cpp:362] mnist -> label
I1129 23:26:11.819401   685 net.cpp:98] Setting up mnist
I1129 23:26:11.819409   685 hdf5_data_layer.cpp:61] Loading filename from ../../data/mnist/table1/10k_split6_train.txt
I1129 23:26:11.819528   685 hdf5_data_layer.cpp:73] Number of files: 5
I1129 23:26:11.819546   685 hdf5_data_layer.cpp:76] Loading HDF5 file/root/si-convnet/data/mnist/table1/mnist-sc-uniform-10ksplit-set-1.h5
I1129 23:26:11.849071   685 hdf5_data_layer.cpp:86] output data size: 100,1,28,28
I1129 23:26:11.849133   685 net.cpp:105] Top shape: 100 1 28 28 (78400)
I1129 23:26:11.849146   685 net.cpp:105] Top shape: 100 1 1 1 (100)
I1129 23:26:11.849170   685 net.cpp:69] Creating Layer label_mnist_1_split
I1129 23:26:11.849182   685 net.cpp:400] label_mnist_1_split <- label
I1129 23:26:11.849197   685 net.cpp:362] label_mnist_1_split -> label_mnist_1_split_0
I1129 23:26:11.849225   685 net.cpp:362] label_mnist_1_split -> label_mnist_1_split_1
I1129 23:26:11.849242   685 net.cpp:98] Setting up label_mnist_1_split
I1129 23:26:11.849254   685 net.cpp:105] Top shape: 100 1 1 1 (100)
I1129 23:26:11.849264   685 net.cpp:105] Top shape: 100 1 1 1 (100)
I1129 23:26:11.849282   685 net.cpp:69] Creating Layer conv1
I1129 23:26:11.849292   685 net.cpp:400] conv1 <- data
I1129 23:26:11.849304   685 net.cpp:362] conv1 -> conv1
I1129 23:26:11.849364   685 net.cpp:98] Setting up conv1
I1129 23:26:11.849380   685 ticonv_layer.cpp:27] TIConvolution layer using 6 transformations conv1 using interpolation: 1
I1129 23:26:11.849390   685 ticonv_layer.cpp:33]  T0 :  sc: 1, rot: 0
I1129 23:26:11.849428   685 ticonv_layer.cpp:33]  T1 :  sc: 0.63, rot: 0
I1129 23:26:11.849443   685 ticonv_layer.cpp:33]  T2 :  sc: 0.7937, rot: 0
I1129 23:26:11.849457   685 ticonv_layer.cpp:33]  T3 :  sc: 1.2599, rot: 0
I1129 23:26:11.849468   685 ticonv_layer.cpp:33]  T4 :  sc: 1.5874, rot: 0
I1129 23:26:11.849480   685 ticonv_layer.cpp:33]  T5 :  sc: 2, rot: 0
I1129 23:26:11.849493   685 ticonv_layer.cpp:37]   Creating Upsampling Layer in conv1
I1129 23:26:11.850373   685 ticonv_layer.cpp:51]   Top shape: 1 28 28
I1129 23:26:11.850394   685 ticonv_layer.cpp:51]   Top shape: 1 17 17
I1129 23:26:11.850404   685 ticonv_layer.cpp:51]   Top shape: 1 22 22
I1129 23:26:11.850411   685 ticonv_layer.cpp:51]   Top shape: 1 35 35
I1129 23:26:11.850419   685 ticonv_layer.cpp:51]   Top shape: 1 44 44
I1129 23:26:11.850427   685 ticonv_layer.cpp:51]   Top shape: 1 56 56
I1129 23:26:11.850435   685 ticonv_layer.cpp:58]   Creating TiedConv Layer in conv1
I1129 23:26:11.850584   685 ticonv_layer.cpp:70]   Top shape: 36 22 22
I1129 23:26:11.850598   685 ticonv_layer.cpp:70]   Top shape: 36 11 11
I1129 23:26:11.850607   685 ticonv_layer.cpp:70]   Top shape: 36 16 16
I1129 23:26:11.850615   685 ticonv_layer.cpp:70]   Top shape: 36 29 29
I1129 23:26:11.850623   685 ticonv_layer.cpp:70]   Top shape: 36 38 38
I1129 23:26:11.850631   685 ticonv_layer.cpp:70]   Top shape: 36 50 50
I1129 23:26:11.850641   685 ticonv_layer.cpp:82]   Creating DownPooling Layer in conv1
I1129 23:26:11.851104   685 net.cpp:105] Top shape: 100 36 22 22 (1742400)
I1129 23:26:11.851136   685 net.cpp:69] Creating Layer relu1
I1129 23:26:11.851150   685 net.cpp:400] relu1 <- conv1
I1129 23:26:11.851163   685 net.cpp:351] relu1 -> conv1 (in-place)
I1129 23:26:11.851176   685 net.cpp:98] Setting up relu1
I1129 23:26:11.851186   685 net.cpp:105] Top shape: 100 36 22 22 (1742400)
I1129 23:26:11.851200   685 net.cpp:69] Creating Layer pool1
I1129 23:26:11.851209   685 net.cpp:400] pool1 <- conv1
I1129 23:26:11.851220   685 net.cpp:362] pool1 -> pool1
I1129 23:26:11.851233   685 net.cpp:98] Setting up pool1
I1129 23:26:11.851246   685 net.cpp:105] Top shape: 100 36 11 11 (435600)
I1129 23:26:11.851261   685 net.cpp:69] Creating Layer conv2
I1129 23:26:11.851270   685 net.cpp:400] conv2 <- pool1
I1129 23:26:11.851282   685 net.cpp:362] conv2 -> conv2
I1129 23:26:11.851296   685 net.cpp:98] Setting up conv2
I1129 23:26:11.851305   685 ticonv_layer.cpp:27] TIConvolution layer using 6 transformations conv2 using interpolation: 1
I1129 23:26:11.851315   685 ticonv_layer.cpp:33]  T0 :  sc: 1, rot: 0
I1129 23:26:11.851331   685 ticonv_layer.cpp:33]  T1 :  sc: 0.63, rot: 0
I1129 23:26:11.851344   685 ticonv_layer.cpp:33]  T2 :  sc: 0.7937, rot: 0
I1129 23:26:11.851356   685 ticonv_layer.cpp:33]  T3 :  sc: 1.2599, rot: 0
I1129 23:26:11.851368   685 ticonv_layer.cpp:33]  T4 :  sc: 1.5874, rot: 0
I1129 23:26:11.851380   685 ticonv_layer.cpp:33]  T5 :  sc: 2, rot: 0
I1129 23:26:11.851392   685 ticonv_layer.cpp:37]   Creating Upsampling Layer in conv2
I1129 23:26:11.851723   685 ticonv_layer.cpp:51]   Top shape: 36 11 11
I1129 23:26:11.851740   685 ticonv_layer.cpp:51]   Top shape: 36 6 6
I1129 23:26:11.851749   685 ticonv_layer.cpp:51]   Top shape: 36 8 8
I1129 23:26:11.851758   685 ticonv_layer.cpp:51]   Top shape: 36 13 13
I1129 23:26:11.851766   685 ticonv_layer.cpp:51]   Top shape: 36 17 17
I1129 23:26:11.851774   685 ticonv_layer.cpp:51]   Top shape: 36 22 22
I1129 23:26:11.851783   685 ticonv_layer.cpp:58]   Creating TiedConv Layer in conv2
I1129 23:26:11.855103   685 ticonv_layer.cpp:70]   Top shape: 64 7 7
I1129 23:26:11.855123   685 ticonv_layer.cpp:70]   Top shape: 64 2 2
I1129 23:26:11.855132   685 ticonv_layer.cpp:70]   Top shape: 64 4 4
I1129 23:26:11.855140   685 ticonv_layer.cpp:70]   Top shape: 64 9 9
I1129 23:26:11.855183   685 ticonv_layer.cpp:70]   Top shape: 64 13 13
I1129 23:26:11.855193   685 ticonv_layer.cpp:70]   Top shape: 64 18 18
I1129 23:26:11.855202   685 ticonv_layer.cpp:82]   Creating DownPooling Layer in conv2
I1129 23:26:11.855463   685 net.cpp:105] Top shape: 100 64 7 7 (313600)
I1129 23:26:11.855490   685 net.cpp:69] Creating Layer relu2
I1129 23:26:11.855500   685 net.cpp:400] relu2 <- conv2
I1129 23:26:11.855512   685 net.cpp:351] relu2 -> conv2 (in-place)
I1129 23:26:11.855525   685 net.cpp:98] Setting up relu2
I1129 23:26:11.855535   685 net.cpp:105] Top shape: 100 64 7 7 (313600)
I1129 23:26:11.855546   685 net.cpp:69] Creating Layer pool2
I1129 23:26:11.855554   685 net.cpp:400] pool2 <- conv2
I1129 23:26:11.855566   685 net.cpp:362] pool2 -> pool2
I1129 23:26:11.855578   685 net.cpp:98] Setting up pool2
I1129 23:26:11.855589   685 net.cpp:105] Top shape: 100 64 3 3 (57600)
I1129 23:26:11.855604   685 net.cpp:69] Creating Layer ip1
I1129 23:26:11.855618   685 net.cpp:400] ip1 <- pool2
I1129 23:26:11.855630   685 net.cpp:362] ip1 -> ip1
I1129 23:26:11.855645   685 net.cpp:98] Setting up ip1
I1129 23:26:11.860900   685 net.cpp:105] Top shape: 100 150 1 1 (15000)
I1129 23:26:11.860930   685 net.cpp:69] Creating Layer relu3
I1129 23:26:11.860945   685 net.cpp:400] relu3 <- ip1
I1129 23:26:11.860957   685 net.cpp:351] relu3 -> ip1 (in-place)
I1129 23:26:11.860970   685 net.cpp:98] Setting up relu3
I1129 23:26:11.860978   685 net.cpp:105] Top shape: 100 150 1 1 (15000)
I1129 23:26:11.860991   685 net.cpp:69] Creating Layer ip2
I1129 23:26:11.861001   685 net.cpp:400] ip2 <- ip1
I1129 23:26:11.861011   685 net.cpp:362] ip2 -> ip2
I1129 23:26:11.861024   685 net.cpp:98] Setting up ip2
I1129 23:26:11.861132   685 net.cpp:105] Top shape: 100 10 1 1 (1000)
I1129 23:26:11.861153   685 net.cpp:69] Creating Layer ip2_ip2_0_split
I1129 23:26:11.861163   685 net.cpp:400] ip2_ip2_0_split <- ip2
I1129 23:26:11.861176   685 net.cpp:362] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1129 23:26:11.861191   685 net.cpp:362] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1129 23:26:11.861203   685 net.cpp:98] Setting up ip2_ip2_0_split
I1129 23:26:11.861213   685 net.cpp:105] Top shape: 100 10 1 1 (1000)
I1129 23:26:11.861223   685 net.cpp:105] Top shape: 100 10 1 1 (1000)
I1129 23:26:11.861236   685 net.cpp:69] Creating Layer accuracy
I1129 23:26:11.861244   685 net.cpp:400] accuracy <- ip2_ip2_0_split_0
I1129 23:26:11.861255   685 net.cpp:400] accuracy <- label_mnist_1_split_0
I1129 23:26:11.861268   685 net.cpp:362] accuracy -> accuracy
I1129 23:26:11.861280   685 net.cpp:98] Setting up accuracy
I1129 23:26:11.861290   685 net.cpp:105] Top shape: 1 1 1 1 (1)
I1129 23:26:11.861304   685 net.cpp:69] Creating Layer loss
I1129 23:26:11.861312   685 net.cpp:400] loss <- ip2_ip2_0_split_1
I1129 23:26:11.861322   685 net.cpp:400] loss <- label_mnist_1_split_1
I1129 23:26:11.861333   685 net.cpp:362] loss -> loss
I1129 23:26:11.861346   685 net.cpp:98] Setting up loss
I1129 23:26:11.861361   685 net.cpp:105] Top shape: 1 1 1 1 (1)
I1129 23:26:11.861369   685 net.cpp:112]     with loss weight 1
I1129 23:26:11.861390   685 net.cpp:175] loss needs backward computation.
I1129 23:26:11.861402   685 net.cpp:177] accuracy does not need backward computation.
I1129 23:26:11.861409   685 net.cpp:175] ip2_ip2_0_split needs backward computation.
I1129 23:26:11.861418   685 net.cpp:175] ip2 needs backward computation.
I1129 23:26:11.861428   685 net.cpp:175] relu3 needs backward computation.
I1129 23:26:11.861435   685 net.cpp:175] ip1 needs backward computation.
I1129 23:26:11.861443   685 net.cpp:175] pool2 needs backward computation.
I1129 23:26:11.861452   685 net.cpp:175] relu2 needs backward computation.
I1129 23:26:11.861460   685 net.cpp:175] conv2 needs backward computation.
I1129 23:26:11.861469   685 net.cpp:175] pool1 needs backward computation.
I1129 23:26:11.861477   685 net.cpp:175] relu1 needs backward computation.
I1129 23:26:11.861485   685 net.cpp:175] conv1 needs backward computation.
I1129 23:26:11.861495   685 net.cpp:177] label_mnist_1_split does not need backward computation.
I1129 23:26:11.861537   685 net.cpp:177] mnist does not need backward computation.
I1129 23:26:11.861547   685 net.cpp:214] This network produces output accuracy
I1129 23:26:11.861557   685 net.cpp:214] This network produces output loss
I1129 23:26:11.861582   685 net.cpp:473] Collecting Learning Rate and Weight Decay.
I1129 23:26:11.861596   685 net.cpp:225] Network initialization done.
I1129 23:26:11.861604   685 net.cpp:226] Memory required for data: 18867628
I1129 23:26:11.861685   685 solver.cpp:41] base lr = 0.01 global weight decay = 0.0001 momentum = 0.9
I1129 23:26:11.861711   685 solver.cpp:44] Solver scaffolding done.
I1129 23:26:11.861721   685 solver.cpp:162] Solving MNIST-sicnn-Table-1-split-6
I1129 23:32:50.720010   685 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 20.97% T1: 19.02% T2: 15.75% T3: 10.23% T4: 4.28% T5: 29.75% 
I1129 23:32:50.720165   685 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 27.20% T1: 4.78% T2: 9.42% T3: 3.26% T4: 31.16% T5: 24.18% 
I1129 23:32:50.720175   685 solver.cpp:223] Epoch 50 (iteration 3900), loss = 0.000756725
I1129 23:32:50.720196   685 solver.cpp:240]     Train net output #0: loss = 0.000756725 (* 1 = 0.000756725 loss)
I1129 23:39:31.007606   685 solver.cpp:284] Epoch 100, Testing net (#0)
I1129 23:39:35.003379   685 solver.cpp:337]     Test net output #0: accuracy = 0.9656 (3.44% error)
I1129 23:39:35.003471   685 solver.cpp:341]     Test net output #1: loss = 0.178451 (* 1 = 0.178451 loss)
I1129 23:39:35.003484   685 solver.cpp:355] Best score: 0.0344 at Epoch 100
I1129 23:39:35.007536   685 solver.cpp:394] Snapshotting solver state to snapshot/sicnn_train10k_mnist-sc_split6_best_96.56.solverstate
I1129 23:39:35.101724   685 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 21.30% T1: 19.05% T2: 15.32% T3: 10.63% T4: 4.40% T5: 29.30% 
I1129 23:39:35.101753   685 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 27.13% T1: 4.77% T2: 9.30% T3: 3.09% T4: 31.32% T5: 24.39% 
I1129 23:39:35.101774   685 solver.cpp:223] Epoch 100 (iteration 7800), loss = 0.000942059
I1129 23:39:35.101795   685 solver.cpp:240]     Train net output #0: loss = 0.000942059 (* 1 = 0.000942059 loss)
I1129 23:46:15.447176   685 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 21.19% T1: 19.05% T2: 15.66% T3: 10.37% T4: 4.32% T5: 29.42% 
I1129 23:46:15.447283   685 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 27.24% T1: 4.51% T2: 9.34% T3: 3.24% T4: 30.95% T5: 24.72% 
I1129 23:46:15.447291   685 solver.cpp:223] Epoch 150 (iteration 11700), loss = 0.000898822
I1129 23:46:15.447312   685 solver.cpp:240]     Train net output #0: loss = 0.000898822 (* 1 = 0.000898822 loss)
I1129 23:52:55.197969   685 solver.cpp:386] Snapshotting to snapshot/sicnn_train10k_mnist-sc_split6_epoch_200.caffemodel
I1129 23:52:55.200354   685 solver.cpp:394] Snapshotting solver state to snapshot/sicnn_train10k_mnist-sc_split6_epoch_200.solverstate
I1129 23:52:55.201371   685 solver.cpp:284] Epoch 200, Testing net (#0)
I1129 23:52:59.082389   685 solver.cpp:337]     Test net output #0: accuracy = 0.9671 (3.28998% error)
I1129 23:52:59.082484   685 solver.cpp:341]     Test net output #1: loss = 0.17678 (* 1 = 0.17678 loss)
I1129 23:52:59.176798   685 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 20.85% T1: 19.68% T2: 15.63% T3: 9.86% T4: 4.01% T5: 29.97% 
I1129 23:52:59.176829   685 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 27.62% T1: 4.69% T2: 9.57% T3: 3.26% T4: 29.91% T5: 24.95% 
I1129 23:52:59.176836   685 solver.cpp:223] Epoch 200 (iteration 15600), loss = 0.000430125
I1129 23:52:59.176869   685 solver.cpp:240]     Train net output #0: loss = 0.000430125 (* 1 = 0.000430125 loss)
I1129 23:59:37.859735   685 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 21.37% T1: 19.23% T2: 15.68% T3: 10.15% T4: 4.35% T5: 29.21% 
I1129 23:59:37.859861   685 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 27.51% T1: 4.63% T2: 9.38% T3: 3.27% T4: 30.27% T5: 24.95% 
I1129 23:59:37.859872   685 solver.cpp:223] Epoch 250 (iteration 19500), loss = 0.000270326
I1129 23:59:37.859895   685 solver.cpp:240]     Train net output #0: loss = 0.000270326 (* 1 = 0.000270326 loss)
I1130 00:06:17.600356   685 solver.cpp:284] Epoch 300, Testing net (#0)
I1130 00:06:21.579602   685 solver.cpp:337]     Test net output #0: accuracy = 0.9674 (3.25998% error)
I1130 00:06:21.579692   685 solver.cpp:341]     Test net output #1: loss = 0.173591 (* 1 = 0.173591 loss)
I1130 00:06:21.674980   685 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 20.92% T1: 19.39% T2: 15.87% T3: 10.13% T4: 4.10% T5: 29.59% 
I1130 00:06:21.675027   685 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 27.36% T1: 4.52% T2: 9.24% T3: 3.21% T4: 30.64% T5: 25.04% 
I1130 00:06:21.675035   685 solver.cpp:223] Epoch 300 (iteration 23400), loss = 0.000478978
I1130 00:06:21.675053   685 solver.cpp:240]     Train net output #0: loss = 0.000478978 (* 1 = 0.000478978 loss)
I1130 00:13:04.538718   685 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 21.02% T1: 19.40% T2: 15.81% T3: 10.00% T4: 4.18% T5: 29.61% 
I1130 00:13:04.538833   685 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 27.56% T1: 4.56% T2: 9.23% T3: 3.24% T4: 30.38% T5: 25.03% 
I1130 00:13:04.538843   685 solver.cpp:223] Epoch 350 (iteration 27300), loss = 0.000632784
I1130 00:13:04.538867   685 solver.cpp:240]     Train net output #0: loss = 0.000632784 (* 1 = 0.000632784 loss)
I1130 00:19:42.771148   685 solver.cpp:386] Snapshotting to snapshot/sicnn_train10k_mnist-sc_split6_epoch_400.caffemodel
I1130 00:19:42.773443   685 solver.cpp:394] Snapshotting solver state to snapshot/sicnn_train10k_mnist-sc_split6_epoch_400.solverstate
I1130 00:19:42.774426   685 solver.cpp:284] Epoch 400, Testing net (#0)
I1130 00:19:46.718590   685 solver.cpp:337]     Test net output #0: accuracy = 0.9645 (3.54999% error)
I1130 00:19:46.718688   685 solver.cpp:341]     Test net output #1: loss = 0.182793 (* 1 = 0.182793 loss)
I1130 00:19:46.718699   685 solver.cpp:355] Best score: 0.0354999 at Epoch 400
I1130 00:19:46.720649   685 solver.cpp:394] Snapshotting solver state to snapshot/sicnn_train10k_mnist-sc_split6_best_96.45.solverstate
I1130 00:19:46.819664   685 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 20.94% T1: 19.34% T2: 15.74% T3: 10.01% T4: 4.29% T5: 29.68% 
I1130 00:19:46.819711   685 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 27.39% T1: 4.59% T2: 9.31% T3: 3.21% T4: 30.33% T5: 25.17% 
I1130 00:19:46.819720   685 solver.cpp:223] Epoch 400 (iteration 31200), loss = 0.00139177
I1130 00:19:46.819741   685 solver.cpp:240]     Train net output #0: loss = 0.00139177 (* 1 = 0.00139177 loss)
I1130 00:26:23.864745   685 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 20.78% T1: 19.49% T2: 15.71% T3: 10.09% T4: 4.08% T5: 29.85% 
I1130 00:26:23.864869   685 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 27.08% T1: 4.44% T2: 9.11% T3: 3.23% T4: 30.85% T5: 25.29% 
I1130 00:26:23.864878   685 solver.cpp:223] Epoch 450 (iteration 35100), loss = 0.00152353
I1130 00:26:23.864900   685 solver.cpp:240]     Train net output #0: loss = 0.00152353 (* 1 = 0.00152353 loss)
I1130 00:33:05.103067   685 solver.cpp:284] Epoch 500, Testing net (#0)
I1130 00:33:08.983144   685 solver.cpp:337]     Test net output #0: accuracy = 0.9659 (3.40998% error)
I1130 00:33:08.983239   685 solver.cpp:341]     Test net output #1: loss = 0.162814 (* 1 = 0.162814 loss)
I1130 00:33:09.071748   685 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 21.09% T1: 19.32% T2: 15.64% T3: 10.08% T4: 4.34% T5: 29.53% 
I1130 00:33:09.071791   685 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 27.04% T1: 4.54% T2: 9.36% T3: 3.20% T4: 30.65% T5: 25.20% 
I1130 00:33:09.071799   685 solver.cpp:223] Epoch 500 (iteration 39000), loss = 0.00118285
I1130 00:33:09.071817   685 solver.cpp:240]     Train net output #0: loss = 0.00118285 (* 1 = 0.00118285 loss)
I1130 00:39:48.593087   685 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 21.28% T1: 19.36% T2: 15.39% T3: 10.22% T4: 4.41% T5: 29.33% 
I1130 00:39:48.593219   685 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 27.24% T1: 4.69% T2: 9.39% T3: 3.20% T4: 30.31% T5: 25.17% 
I1130 00:39:48.593230   685 solver.cpp:223] Epoch 550 (iteration 42900), loss = 0.000535429
I1130 00:39:48.593252   685 solver.cpp:240]     Train net output #0: loss = 0.000535429 (* 1 = 0.000535429 loss)
I1130 00:46:28.698932   685 solver.cpp:386] Snapshotting to snapshot/sicnn_train10k_mnist-sc_split6_epoch_600.caffemodel
I1130 00:46:28.701269   685 solver.cpp:394] Snapshotting solver state to snapshot/sicnn_train10k_mnist-sc_split6_epoch_600.solverstate
I1130 00:46:28.702250   685 solver.cpp:284] Epoch 600, Testing net (#0)
I1130 00:46:32.668704   685 solver.cpp:337]     Test net output #0: accuracy = 0.9696 (3.03999% error)
I1130 00:46:32.668797   685 solver.cpp:341]     Test net output #1: loss = 0.157305 (* 1 = 0.157305 loss)
I1130 00:46:32.762935   685 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 20.86% T1: 19.44% T2: 15.60% T3: 10.03% T4: 4.22% T5: 29.86% 
I1130 00:46:32.762981   685 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 27.30% T1: 4.49% T2: 9.15% T3: 3.09% T4: 30.57% T5: 25.40% 
I1130 00:46:32.762989   685 solver.cpp:223] Epoch 600 (iteration 46800), loss = 0.00224068
I1130 00:46:32.763007   685 solver.cpp:240]     Train net output #0: loss = 0.00224068 (* 1 = 0.00224068 loss)
I1130 00:53:14.818754   685 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 21.19% T1: 19.22% T2: 15.67% T3: 10.30% T4: 4.33% T5: 29.31% 
I1130 00:53:14.818861   685 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 27.03% T1: 4.39% T2: 8.77% T3: 3.19% T4: 31.12% T5: 25.50% 
I1130 00:53:14.818871   685 solver.cpp:223] Epoch 650 (iteration 50700), loss = 0.000185797
I1130 00:53:14.818892   685 solver.cpp:240]     Train net output #0: loss = 0.000185797 (* 1 = 0.000185797 loss)
I1130 00:59:52.778282   685 solver.cpp:284] Epoch 700, Testing net (#0)
I1130 00:59:56.733644   685 solver.cpp:337]     Test net output #0: accuracy = 0.9677 (3.22998% error)
I1130 00:59:56.733728   685 solver.cpp:341]     Test net output #1: loss = 0.158928 (* 1 = 0.158928 loss)
I1130 00:59:56.826197   685 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 20.80% T1: 19.21% T2: 15.81% T3: 10.24% T4: 4.07% T5: 29.87% 
I1130 00:59:56.826232   685 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 26.78% T1: 4.17% T2: 8.67% T3: 3.21% T4: 31.41% T5: 25.75% 
I1130 00:59:56.826253   685 solver.cpp:223] Epoch 700 (iteration 54600), loss = 0.000235252
I1130 00:59:56.826272   685 solver.cpp:240]     Train net output #0: loss = 0.000235252 (* 1 = 0.000235252 loss)
I1130 00:59:56.836396   685 solver.cpp:386] Snapshotting to snapshot/sicnn_train10k_mnist-sc_split6_epoch_700.caffemodel
I1130 00:59:56.838532   685 solver.cpp:394] Snapshotting solver state to snapshot/sicnn_train10k_mnist-sc_split6_epoch_700.solverstate
I1130 00:59:56.839515   685 solver.cpp:270] Optimization Done.
I1130 00:59:56.839529   685 caffe.cpp:124] Optimization Done.
