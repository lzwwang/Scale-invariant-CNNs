I1129 20:05:05.268127  1575 caffe.cpp:102] Use GPU with device ID 0
I1129 20:05:05.270321  1575 caffe.cpp:110] Starting Optimization
I1129 20:05:05.270424  1575 solver.cpp:32] Initializing solver from parameters: 
test_iter: 100
test_interval: 100
base_lr: 0.01
display: 50
max_iter: 700
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 200
snapshot_prefix: "snapshot/sicnn_train10k_mnist-sc_split4"
solver_mode: GPU
net: "protos/sicnn_train10k_train_test_mnist-sc_split4.prototxt"
epoch_size: 78
save_max: true
I1129 20:05:05.270460  1575 solver.cpp:71] Creating training net from net file: protos/sicnn_train10k_train_test_mnist-sc_split4.prototxt
I1129 20:05:05.271023  1575 net.cpp:281] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1129 20:05:05.271059  1575 net.cpp:281] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1129 20:05:05.271262  1575 net.cpp:41] Initializing net from parameters: 
name: "MNIST-sicnn-Table-1-split-4"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: HDF5_DATA
  hdf5_data_param {
    source: "../../data/mnist/table1/10k_split4_test.txt"
    batch_size: 128
  }
  include {
    phase: TRAIN
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: TICONV
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 36
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
  transformations {
  }
  transformations {
    scale: 0.63
  }
  transformations {
    scale: 0.7937
  }
  transformations {
    scale: 1.2599
  }
  transformations {
    scale: 1.5874
  }
  transformations {
    scale: 2
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: TICONV
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 64
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
  transformations {
  }
  transformations {
    scale: 0.63
  }
  transformations {
    scale: 0.7937
  }
  transformations {
    scale: 1.2599
  }
  transformations {
    scale: 1.5874
  }
  transformations {
    scale: 2
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layers {
  bottom: "pool2"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 150
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1129 20:05:05.271366  1575 net.cpp:69] Creating Layer mnist
I1129 20:05:05.271383  1575 net.cpp:362] mnist -> data
I1129 20:05:05.271404  1575 net.cpp:362] mnist -> label
I1129 20:05:05.271418  1575 net.cpp:98] Setting up mnist
I1129 20:05:05.271428  1575 hdf5_data_layer.cpp:61] Loading filename from ../../data/mnist/table1/10k_split4_test.txt
I1129 20:05:05.277880  1575 hdf5_data_layer.cpp:73] Number of files: 1
I1129 20:05:05.277910  1575 hdf5_data_layer.cpp:76] Loading HDF5 file/root/si-convnet/data/mnist/table1/mnist-sc-uniform-10ksplit-set-4.h5
I1129 20:05:05.310945  1575 hdf5_data_layer.cpp:86] output data size: 128,1,28,28
I1129 20:05:05.311081  1575 net.cpp:105] Top shape: 128 1 28 28 (100352)
I1129 20:05:05.311096  1575 net.cpp:105] Top shape: 128 1 1 1 (128)
I1129 20:05:05.311123  1575 net.cpp:69] Creating Layer conv1
I1129 20:05:05.311134  1575 net.cpp:400] conv1 <- data
I1129 20:05:05.311152  1575 net.cpp:362] conv1 -> conv1
I1129 20:05:05.311173  1575 net.cpp:98] Setting up conv1
I1129 20:05:05.311200  1575 ticonv_layer.cpp:27] TIConvolution layer using 6 transformations conv1 using interpolation: 1
I1129 20:05:05.311214  1575 ticonv_layer.cpp:33]  T0 :  sc: 1, rot: 0
I1129 20:05:05.311288  1575 ticonv_layer.cpp:33]  T1 :  sc: 0.63, rot: 0
I1129 20:05:05.311305  1575 ticonv_layer.cpp:33]  T2 :  sc: 0.7937, rot: 0
I1129 20:05:05.311318  1575 ticonv_layer.cpp:33]  T3 :  sc: 1.2599, rot: 0
I1129 20:05:05.311331  1575 ticonv_layer.cpp:33]  T4 :  sc: 1.5874, rot: 0
I1129 20:05:05.311343  1575 ticonv_layer.cpp:33]  T5 :  sc: 2, rot: 0
I1129 20:05:05.311357  1575 ticonv_layer.cpp:37]   Creating Upsampling Layer in conv1
I1129 20:05:05.325186  1575 ticonv_layer.cpp:51]   Top shape: 1 28 28
I1129 20:05:05.325212  1575 ticonv_layer.cpp:51]   Top shape: 1 17 17
I1129 20:05:05.325222  1575 ticonv_layer.cpp:51]   Top shape: 1 22 22
I1129 20:05:05.325232  1575 ticonv_layer.cpp:51]   Top shape: 1 35 35
I1129 20:05:05.325239  1575 ticonv_layer.cpp:51]   Top shape: 1 44 44
I1129 20:05:05.325248  1575 ticonv_layer.cpp:51]   Top shape: 1 56 56
I1129 20:05:05.325256  1575 ticonv_layer.cpp:58]   Creating TiedConv Layer in conv1
I1129 20:05:05.325423  1575 ticonv_layer.cpp:70]   Top shape: 36 22 22
I1129 20:05:05.325441  1575 ticonv_layer.cpp:70]   Top shape: 36 11 11
I1129 20:05:05.325450  1575 ticonv_layer.cpp:70]   Top shape: 36 16 16
I1129 20:05:05.325459  1575 ticonv_layer.cpp:70]   Top shape: 36 29 29
I1129 20:05:05.325467  1575 ticonv_layer.cpp:70]   Top shape: 36 38 38
I1129 20:05:05.325476  1575 ticonv_layer.cpp:70]   Top shape: 36 50 50
I1129 20:05:05.325485  1575 ticonv_layer.cpp:82]   Creating DownPooling Layer in conv1
I1129 20:05:05.325960  1575 net.cpp:105] Top shape: 128 36 22 22 (2230272)
I1129 20:05:05.326001  1575 net.cpp:69] Creating Layer relu1
I1129 20:05:05.326016  1575 net.cpp:400] relu1 <- conv1
I1129 20:05:05.326030  1575 net.cpp:351] relu1 -> conv1 (in-place)
I1129 20:05:05.326043  1575 net.cpp:98] Setting up relu1
I1129 20:05:05.326053  1575 net.cpp:105] Top shape: 128 36 22 22 (2230272)
I1129 20:05:05.326066  1575 net.cpp:69] Creating Layer pool1
I1129 20:05:05.326074  1575 net.cpp:400] pool1 <- conv1
I1129 20:05:05.326086  1575 net.cpp:362] pool1 -> pool1
I1129 20:05:05.326100  1575 net.cpp:98] Setting up pool1
I1129 20:05:05.326112  1575 net.cpp:105] Top shape: 128 36 11 11 (557568)
I1129 20:05:05.326129  1575 net.cpp:69] Creating Layer conv2
I1129 20:05:05.326138  1575 net.cpp:400] conv2 <- pool1
I1129 20:05:05.326151  1575 net.cpp:362] conv2 -> conv2
I1129 20:05:05.326167  1575 net.cpp:98] Setting up conv2
I1129 20:05:05.326176  1575 ticonv_layer.cpp:27] TIConvolution layer using 6 transformations conv2 using interpolation: 1
I1129 20:05:05.326185  1575 ticonv_layer.cpp:33]  T0 :  sc: 1, rot: 0
I1129 20:05:05.326205  1575 ticonv_layer.cpp:33]  T1 :  sc: 0.63, rot: 0
I1129 20:05:05.326218  1575 ticonv_layer.cpp:33]  T2 :  sc: 0.7937, rot: 0
I1129 20:05:05.326231  1575 ticonv_layer.cpp:33]  T3 :  sc: 1.2599, rot: 0
I1129 20:05:05.326244  1575 ticonv_layer.cpp:33]  T4 :  sc: 1.5874, rot: 0
I1129 20:05:05.326256  1575 ticonv_layer.cpp:33]  T5 :  sc: 2, rot: 0
I1129 20:05:05.326268  1575 ticonv_layer.cpp:37]   Creating Upsampling Layer in conv2
I1129 20:05:05.326614  1575 ticonv_layer.cpp:51]   Top shape: 36 11 11
I1129 20:05:05.326632  1575 ticonv_layer.cpp:51]   Top shape: 36 6 6
I1129 20:05:05.326642  1575 ticonv_layer.cpp:51]   Top shape: 36 8 8
I1129 20:05:05.326650  1575 ticonv_layer.cpp:51]   Top shape: 36 13 13
I1129 20:05:05.326658  1575 ticonv_layer.cpp:51]   Top shape: 36 17 17
I1129 20:05:05.326666  1575 ticonv_layer.cpp:51]   Top shape: 36 22 22
I1129 20:05:05.326674  1575 ticonv_layer.cpp:58]   Creating TiedConv Layer in conv2
I1129 20:05:05.330148  1575 ticonv_layer.cpp:70]   Top shape: 64 7 7
I1129 20:05:05.330169  1575 ticonv_layer.cpp:70]   Top shape: 64 2 2
I1129 20:05:05.330179  1575 ticonv_layer.cpp:70]   Top shape: 64 4 4
I1129 20:05:05.330188  1575 ticonv_layer.cpp:70]   Top shape: 64 9 9
I1129 20:05:05.330195  1575 ticonv_layer.cpp:70]   Top shape: 64 13 13
I1129 20:05:05.330204  1575 ticonv_layer.cpp:70]   Top shape: 64 18 18
I1129 20:05:05.330212  1575 ticonv_layer.cpp:82]   Creating DownPooling Layer in conv2
I1129 20:05:05.330483  1575 net.cpp:105] Top shape: 128 64 7 7 (401408)
I1129 20:05:05.330513  1575 net.cpp:69] Creating Layer relu2
I1129 20:05:05.330523  1575 net.cpp:400] relu2 <- conv2
I1129 20:05:05.330536  1575 net.cpp:351] relu2 -> conv2 (in-place)
I1129 20:05:05.330549  1575 net.cpp:98] Setting up relu2
I1129 20:05:05.330559  1575 net.cpp:105] Top shape: 128 64 7 7 (401408)
I1129 20:05:05.330570  1575 net.cpp:69] Creating Layer pool2
I1129 20:05:05.330579  1575 net.cpp:400] pool2 <- conv2
I1129 20:05:05.330590  1575 net.cpp:362] pool2 -> pool2
I1129 20:05:05.330603  1575 net.cpp:98] Setting up pool2
I1129 20:05:05.330615  1575 net.cpp:105] Top shape: 128 64 3 3 (73728)
I1129 20:05:05.330628  1575 net.cpp:69] Creating Layer ip1
I1129 20:05:05.330637  1575 net.cpp:400] ip1 <- pool2
I1129 20:05:05.330649  1575 net.cpp:362] ip1 -> ip1
I1129 20:05:05.330663  1575 net.cpp:98] Setting up ip1
I1129 20:05:05.335763  1575 net.cpp:105] Top shape: 128 150 1 1 (19200)
I1129 20:05:05.335798  1575 net.cpp:69] Creating Layer relu3
I1129 20:05:05.335808  1575 net.cpp:400] relu3 <- ip1
I1129 20:05:05.335820  1575 net.cpp:351] relu3 -> ip1 (in-place)
I1129 20:05:05.335832  1575 net.cpp:98] Setting up relu3
I1129 20:05:05.335841  1575 net.cpp:105] Top shape: 128 150 1 1 (19200)
I1129 20:05:05.335855  1575 net.cpp:69] Creating Layer ip2
I1129 20:05:05.335863  1575 net.cpp:400] ip2 <- ip1
I1129 20:05:05.335875  1575 net.cpp:362] ip2 -> ip2
I1129 20:05:05.335888  1575 net.cpp:98] Setting up ip2
I1129 20:05:05.335989  1575 net.cpp:105] Top shape: 128 10 1 1 (1280)
I1129 20:05:05.336009  1575 net.cpp:69] Creating Layer loss
I1129 20:05:05.336019  1575 net.cpp:400] loss <- ip2
I1129 20:05:05.336030  1575 net.cpp:400] loss <- label
I1129 20:05:05.336041  1575 net.cpp:362] loss -> loss
I1129 20:05:05.336055  1575 net.cpp:98] Setting up loss
I1129 20:05:05.336071  1575 net.cpp:105] Top shape: 1 1 1 1 (1)
I1129 20:05:05.336081  1575 net.cpp:112]     with loss weight 1
I1129 20:05:05.336097  1575 net.cpp:175] loss needs backward computation.
I1129 20:05:05.336107  1575 net.cpp:175] ip2 needs backward computation.
I1129 20:05:05.336117  1575 net.cpp:175] relu3 needs backward computation.
I1129 20:05:05.336125  1575 net.cpp:175] ip1 needs backward computation.
I1129 20:05:05.336134  1575 net.cpp:175] pool2 needs backward computation.
I1129 20:05:05.336143  1575 net.cpp:175] relu2 needs backward computation.
I1129 20:05:05.336151  1575 net.cpp:175] conv2 needs backward computation.
I1129 20:05:05.336166  1575 net.cpp:175] pool1 needs backward computation.
I1129 20:05:05.336175  1575 net.cpp:175] relu1 needs backward computation.
I1129 20:05:05.336184  1575 net.cpp:175] conv1 needs backward computation.
I1129 20:05:05.336194  1575 net.cpp:177] mnist does not need backward computation.
I1129 20:05:05.336202  1575 net.cpp:214] This network produces output loss
I1129 20:05:05.336220  1575 net.cpp:473] Collecting Learning Rate and Weight Decay.
I1129 20:05:05.336232  1575 net.cpp:225] Network initialization done.
I1129 20:05:05.336241  1575 net.cpp:226] Memory required for data: 24155976
I1129 20:05:05.336683  1575 solver.cpp:154] Creating test net (#0) specified by net file: protos/sicnn_train10k_train_test_mnist-sc_split4.prototxt
I1129 20:05:05.336740  1575 net.cpp:281] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1129 20:05:05.336969  1575 net.cpp:41] Initializing net from parameters: 
name: "MNIST-sicnn-Table-1-split-4"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: HDF5_DATA
  hdf5_data_param {
    source: "../../data/mnist/table1/10k_split4_train.txt"
    batch_size: 100
  }
  include {
    phase: TEST
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: TICONV
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 36
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
  transformations {
  }
  transformations {
    scale: 0.63
  }
  transformations {
    scale: 0.7937
  }
  transformations {
    scale: 1.2599
  }
  transformations {
    scale: 1.5874
  }
  transformations {
    scale: 2
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: TICONV
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 64
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
  transformations {
  }
  transformations {
    scale: 0.63
  }
  transformations {
    scale: 0.7937
  }
  transformations {
    scale: 1.2599
  }
  transformations {
    scale: 1.5874
  }
  transformations {
    scale: 2
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layers {
  bottom: "pool2"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 150
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  name: "accuracy"
  type: ACCURACY
  include {
    phase: TEST
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TEST
}
I1129 20:05:05.337152  1575 net.cpp:69] Creating Layer mnist
I1129 20:05:05.337169  1575 net.cpp:362] mnist -> data
I1129 20:05:05.337186  1575 net.cpp:362] mnist -> label
I1129 20:05:05.337201  1575 net.cpp:98] Setting up mnist
I1129 20:05:05.337211  1575 hdf5_data_layer.cpp:61] Loading filename from ../../data/mnist/table1/10k_split4_train.txt
I1129 20:05:05.337549  1575 hdf5_data_layer.cpp:73] Number of files: 5
I1129 20:05:05.337569  1575 hdf5_data_layer.cpp:76] Loading HDF5 file/root/si-convnet/data/mnist/table1/mnist-sc-uniform-10ksplit-set-1.h5
I1129 20:05:05.367314  1575 hdf5_data_layer.cpp:86] output data size: 100,1,28,28
I1129 20:05:05.367381  1575 net.cpp:105] Top shape: 100 1 28 28 (78400)
I1129 20:05:05.367396  1575 net.cpp:105] Top shape: 100 1 1 1 (100)
I1129 20:05:05.367420  1575 net.cpp:69] Creating Layer label_mnist_1_split
I1129 20:05:05.367432  1575 net.cpp:400] label_mnist_1_split <- label
I1129 20:05:05.367449  1575 net.cpp:362] label_mnist_1_split -> label_mnist_1_split_0
I1129 20:05:05.367468  1575 net.cpp:362] label_mnist_1_split -> label_mnist_1_split_1
I1129 20:05:05.367482  1575 net.cpp:98] Setting up label_mnist_1_split
I1129 20:05:05.367496  1575 net.cpp:105] Top shape: 100 1 1 1 (100)
I1129 20:05:05.367506  1575 net.cpp:105] Top shape: 100 1 1 1 (100)
I1129 20:05:05.367522  1575 net.cpp:69] Creating Layer conv1
I1129 20:05:05.367532  1575 net.cpp:400] conv1 <- data
I1129 20:05:05.367547  1575 net.cpp:362] conv1 -> conv1
I1129 20:05:05.367607  1575 net.cpp:98] Setting up conv1
I1129 20:05:05.367619  1575 ticonv_layer.cpp:27] TIConvolution layer using 6 transformations conv1 using interpolation: 1
I1129 20:05:05.367630  1575 ticonv_layer.cpp:33]  T0 :  sc: 1, rot: 0
I1129 20:05:05.367669  1575 ticonv_layer.cpp:33]  T1 :  sc: 0.63, rot: 0
I1129 20:05:05.367686  1575 ticonv_layer.cpp:33]  T2 :  sc: 0.7937, rot: 0
I1129 20:05:05.367698  1575 ticonv_layer.cpp:33]  T3 :  sc: 1.2599, rot: 0
I1129 20:05:05.367712  1575 ticonv_layer.cpp:33]  T4 :  sc: 1.5874, rot: 0
I1129 20:05:05.367723  1575 ticonv_layer.cpp:33]  T5 :  sc: 2, rot: 0
I1129 20:05:05.367736  1575 ticonv_layer.cpp:37]   Creating Upsampling Layer in conv1
I1129 20:05:05.368639  1575 ticonv_layer.cpp:51]   Top shape: 1 28 28
I1129 20:05:05.368660  1575 ticonv_layer.cpp:51]   Top shape: 1 17 17
I1129 20:05:05.368670  1575 ticonv_layer.cpp:51]   Top shape: 1 22 22
I1129 20:05:05.368680  1575 ticonv_layer.cpp:51]   Top shape: 1 35 35
I1129 20:05:05.368687  1575 ticonv_layer.cpp:51]   Top shape: 1 44 44
I1129 20:05:05.368697  1575 ticonv_layer.cpp:51]   Top shape: 1 56 56
I1129 20:05:05.368705  1575 ticonv_layer.cpp:58]   Creating TiedConv Layer in conv1
I1129 20:05:05.368854  1575 ticonv_layer.cpp:70]   Top shape: 36 22 22
I1129 20:05:05.368870  1575 ticonv_layer.cpp:70]   Top shape: 36 11 11
I1129 20:05:05.368878  1575 ticonv_layer.cpp:70]   Top shape: 36 16 16
I1129 20:05:05.368887  1575 ticonv_layer.cpp:70]   Top shape: 36 29 29
I1129 20:05:05.368896  1575 ticonv_layer.cpp:70]   Top shape: 36 38 38
I1129 20:05:05.368904  1575 ticonv_layer.cpp:70]   Top shape: 36 50 50
I1129 20:05:05.368913  1575 ticonv_layer.cpp:82]   Creating DownPooling Layer in conv1
I1129 20:05:05.369362  1575 net.cpp:105] Top shape: 100 36 22 22 (1742400)
I1129 20:05:05.369396  1575 net.cpp:69] Creating Layer relu1
I1129 20:05:05.369407  1575 net.cpp:400] relu1 <- conv1
I1129 20:05:05.369419  1575 net.cpp:351] relu1 -> conv1 (in-place)
I1129 20:05:05.369432  1575 net.cpp:98] Setting up relu1
I1129 20:05:05.369442  1575 net.cpp:105] Top shape: 100 36 22 22 (1742400)
I1129 20:05:05.369457  1575 net.cpp:69] Creating Layer pool1
I1129 20:05:05.369465  1575 net.cpp:400] pool1 <- conv1
I1129 20:05:05.369477  1575 net.cpp:362] pool1 -> pool1
I1129 20:05:05.369490  1575 net.cpp:98] Setting up pool1
I1129 20:05:05.369503  1575 net.cpp:105] Top shape: 100 36 11 11 (435600)
I1129 20:05:05.369518  1575 net.cpp:69] Creating Layer conv2
I1129 20:05:05.369526  1575 net.cpp:400] conv2 <- pool1
I1129 20:05:05.369539  1575 net.cpp:362] conv2 -> conv2
I1129 20:05:05.369554  1575 net.cpp:98] Setting up conv2
I1129 20:05:05.369562  1575 ticonv_layer.cpp:27] TIConvolution layer using 6 transformations conv2 using interpolation: 1
I1129 20:05:05.369571  1575 ticonv_layer.cpp:33]  T0 :  sc: 1, rot: 0
I1129 20:05:05.369596  1575 ticonv_layer.cpp:33]  T1 :  sc: 0.63, rot: 0
I1129 20:05:05.369609  1575 ticonv_layer.cpp:33]  T2 :  sc: 0.7937, rot: 0
I1129 20:05:05.369622  1575 ticonv_layer.cpp:33]  T3 :  sc: 1.2599, rot: 0
I1129 20:05:05.369634  1575 ticonv_layer.cpp:33]  T4 :  sc: 1.5874, rot: 0
I1129 20:05:05.369647  1575 ticonv_layer.cpp:33]  T5 :  sc: 2, rot: 0
I1129 20:05:05.369658  1575 ticonv_layer.cpp:37]   Creating Upsampling Layer in conv2
I1129 20:05:05.369976  1575 ticonv_layer.cpp:51]   Top shape: 36 11 11
I1129 20:05:05.369993  1575 ticonv_layer.cpp:51]   Top shape: 36 6 6
I1129 20:05:05.370002  1575 ticonv_layer.cpp:51]   Top shape: 36 8 8
I1129 20:05:05.370012  1575 ticonv_layer.cpp:51]   Top shape: 36 13 13
I1129 20:05:05.370019  1575 ticonv_layer.cpp:51]   Top shape: 36 17 17
I1129 20:05:05.370028  1575 ticonv_layer.cpp:51]   Top shape: 36 22 22
I1129 20:05:05.370036  1575 ticonv_layer.cpp:58]   Creating TiedConv Layer in conv2
I1129 20:05:05.373315  1575 ticonv_layer.cpp:70]   Top shape: 64 7 7
I1129 20:05:05.373335  1575 ticonv_layer.cpp:70]   Top shape: 64 2 2
I1129 20:05:05.373344  1575 ticonv_layer.cpp:70]   Top shape: 64 4 4
I1129 20:05:05.373353  1575 ticonv_layer.cpp:70]   Top shape: 64 9 9
I1129 20:05:05.373394  1575 ticonv_layer.cpp:70]   Top shape: 64 13 13
I1129 20:05:05.373402  1575 ticonv_layer.cpp:70]   Top shape: 64 18 18
I1129 20:05:05.373412  1575 ticonv_layer.cpp:82]   Creating DownPooling Layer in conv2
I1129 20:05:05.373664  1575 net.cpp:105] Top shape: 100 64 7 7 (313600)
I1129 20:05:05.373692  1575 net.cpp:69] Creating Layer relu2
I1129 20:05:05.373703  1575 net.cpp:400] relu2 <- conv2
I1129 20:05:05.373714  1575 net.cpp:351] relu2 -> conv2 (in-place)
I1129 20:05:05.373728  1575 net.cpp:98] Setting up relu2
I1129 20:05:05.373736  1575 net.cpp:105] Top shape: 100 64 7 7 (313600)
I1129 20:05:05.373749  1575 net.cpp:69] Creating Layer pool2
I1129 20:05:05.373757  1575 net.cpp:400] pool2 <- conv2
I1129 20:05:05.373769  1575 net.cpp:362] pool2 -> pool2
I1129 20:05:05.373781  1575 net.cpp:98] Setting up pool2
I1129 20:05:05.373792  1575 net.cpp:105] Top shape: 100 64 3 3 (57600)
I1129 20:05:05.373807  1575 net.cpp:69] Creating Layer ip1
I1129 20:05:05.373816  1575 net.cpp:400] ip1 <- pool2
I1129 20:05:05.373828  1575 net.cpp:362] ip1 -> ip1
I1129 20:05:05.373843  1575 net.cpp:98] Setting up ip1
I1129 20:05:05.378958  1575 net.cpp:105] Top shape: 100 150 1 1 (15000)
I1129 20:05:05.378989  1575 net.cpp:69] Creating Layer relu3
I1129 20:05:05.379000  1575 net.cpp:400] relu3 <- ip1
I1129 20:05:05.379012  1575 net.cpp:351] relu3 -> ip1 (in-place)
I1129 20:05:05.379024  1575 net.cpp:98] Setting up relu3
I1129 20:05:05.379034  1575 net.cpp:105] Top shape: 100 150 1 1 (15000)
I1129 20:05:05.379046  1575 net.cpp:69] Creating Layer ip2
I1129 20:05:05.379055  1575 net.cpp:400] ip2 <- ip1
I1129 20:05:05.379067  1575 net.cpp:362] ip2 -> ip2
I1129 20:05:05.379081  1575 net.cpp:98] Setting up ip2
I1129 20:05:05.379187  1575 net.cpp:105] Top shape: 100 10 1 1 (1000)
I1129 20:05:05.379209  1575 net.cpp:69] Creating Layer ip2_ip2_0_split
I1129 20:05:05.379217  1575 net.cpp:400] ip2_ip2_0_split <- ip2
I1129 20:05:05.379230  1575 net.cpp:362] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1129 20:05:05.379246  1575 net.cpp:362] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1129 20:05:05.379259  1575 net.cpp:98] Setting up ip2_ip2_0_split
I1129 20:05:05.379271  1575 net.cpp:105] Top shape: 100 10 1 1 (1000)
I1129 20:05:05.379279  1575 net.cpp:105] Top shape: 100 10 1 1 (1000)
I1129 20:05:05.379293  1575 net.cpp:69] Creating Layer accuracy
I1129 20:05:05.379302  1575 net.cpp:400] accuracy <- ip2_ip2_0_split_0
I1129 20:05:05.379313  1575 net.cpp:400] accuracy <- label_mnist_1_split_0
I1129 20:05:05.379326  1575 net.cpp:362] accuracy -> accuracy
I1129 20:05:05.379339  1575 net.cpp:98] Setting up accuracy
I1129 20:05:05.379349  1575 net.cpp:105] Top shape: 1 1 1 1 (1)
I1129 20:05:05.379364  1575 net.cpp:69] Creating Layer loss
I1129 20:05:05.379372  1575 net.cpp:400] loss <- ip2_ip2_0_split_1
I1129 20:05:05.379382  1575 net.cpp:400] loss <- label_mnist_1_split_1
I1129 20:05:05.379395  1575 net.cpp:362] loss -> loss
I1129 20:05:05.379407  1575 net.cpp:98] Setting up loss
I1129 20:05:05.379422  1575 net.cpp:105] Top shape: 1 1 1 1 (1)
I1129 20:05:05.379431  1575 net.cpp:112]     with loss weight 1
I1129 20:05:05.379448  1575 net.cpp:175] loss needs backward computation.
I1129 20:05:05.379459  1575 net.cpp:177] accuracy does not need backward computation.
I1129 20:05:05.379467  1575 net.cpp:175] ip2_ip2_0_split needs backward computation.
I1129 20:05:05.379477  1575 net.cpp:175] ip2 needs backward computation.
I1129 20:05:05.379485  1575 net.cpp:175] relu3 needs backward computation.
I1129 20:05:05.379493  1575 net.cpp:175] ip1 needs backward computation.
I1129 20:05:05.379503  1575 net.cpp:175] pool2 needs backward computation.
I1129 20:05:05.379513  1575 net.cpp:175] relu2 needs backward computation.
I1129 20:05:05.379520  1575 net.cpp:175] conv2 needs backward computation.
I1129 20:05:05.379529  1575 net.cpp:175] pool1 needs backward computation.
I1129 20:05:05.379539  1575 net.cpp:175] relu1 needs backward computation.
I1129 20:05:05.379546  1575 net.cpp:175] conv1 needs backward computation.
I1129 20:05:05.379555  1575 net.cpp:177] label_mnist_1_split does not need backward computation.
I1129 20:05:05.379596  1575 net.cpp:177] mnist does not need backward computation.
I1129 20:05:05.379606  1575 net.cpp:214] This network produces output accuracy
I1129 20:05:05.379616  1575 net.cpp:214] This network produces output loss
I1129 20:05:05.379642  1575 net.cpp:473] Collecting Learning Rate and Weight Decay.
I1129 20:05:05.379659  1575 net.cpp:225] Network initialization done.
I1129 20:05:05.379667  1575 net.cpp:226] Memory required for data: 18867608
I1129 20:05:05.379747  1575 solver.cpp:41] base lr = 0.01 global weight decay = 0.0001 momentum = 0.9
I1129 20:05:05.379773  1575 solver.cpp:44] Solver scaffolding done.
I1129 20:05:05.379784  1575 solver.cpp:162] Solving MNIST-sicnn-Table-1-split-4
I1129 20:11:42.668774  1575 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 21.23% T1: 19.14% T2: 15.93% T3: 9.98% T4: 4.05% T5: 29.67% 
I1129 20:11:42.668922  1575 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 28.74% T1: 4.91% T2: 9.53% T3: 3.25% T4: 29.88% T5: 23.69% 
I1129 20:11:42.668931  1575 solver.cpp:223] Epoch 50 (iteration 3900), loss = 0.00272302
I1129 20:11:42.668969  1575 solver.cpp:240]     Train net output #0: loss = 0.00272302 (* 1 = 0.00272302 loss)
I1129 20:18:20.181782  1575 solver.cpp:284] Epoch 100, Testing net (#0)
I1129 20:18:24.141127  1575 solver.cpp:337]     Test net output #0: accuracy = 0.9732 (2.67997% error)
I1129 20:18:24.141214  1575 solver.cpp:341]     Test net output #1: loss = 0.139354 (* 1 = 0.139354 loss)
I1129 20:18:24.141224  1575 solver.cpp:355] Best score: 0.0267997 at Epoch 100
I1129 20:18:24.145202  1575 solver.cpp:394] Snapshotting solver state to snapshot/sicnn_train10k_mnist-sc_split4_best_97.32.solverstate
I1129 20:18:24.238144  1575 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 21.28% T1: 19.06% T2: 16.03% T3: 9.96% T4: 4.09% T5: 29.56% 
I1129 20:18:24.238190  1575 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 28.76% T1: 4.51% T2: 9.31% T3: 3.40% T4: 29.94% T5: 24.09% 
I1129 20:18:24.238198  1575 solver.cpp:223] Epoch 100 (iteration 7800), loss = 0.00190463
I1129 20:18:24.238219  1575 solver.cpp:240]     Train net output #0: loss = 0.00190463 (* 1 = 0.00190463 loss)
I1129 20:25:03.952301  1575 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 21.26% T1: 19.00% T2: 16.25% T3: 9.87% T4: 3.99% T5: 29.63% 
I1129 20:25:03.952435  1575 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 28.53% T1: 4.45% T2: 9.43% T3: 3.43% T4: 29.64% T5: 24.52% 
I1129 20:25:03.952445  1575 solver.cpp:223] Epoch 150 (iteration 11700), loss = 0.00217807
I1129 20:25:03.952468  1575 solver.cpp:240]     Train net output #0: loss = 0.00217807 (* 1 = 0.00217807 loss)
I1129 20:31:41.844611  1575 solver.cpp:386] Snapshotting to snapshot/sicnn_train10k_mnist-sc_split4_epoch_200.caffemodel
I1129 20:31:41.847026  1575 solver.cpp:394] Snapshotting solver state to snapshot/sicnn_train10k_mnist-sc_split4_epoch_200.solverstate
I1129 20:31:41.848021  1575 solver.cpp:284] Epoch 200, Testing net (#0)
I1129 20:31:45.730334  1575 solver.cpp:337]     Test net output #0: accuracy = 0.9742 (2.57996% error)
I1129 20:31:45.730432  1575 solver.cpp:341]     Test net output #1: loss = 0.129246 (* 1 = 0.129246 loss)
I1129 20:31:45.828285  1575 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 21.67% T1: 19.15% T2: 15.42% T3: 10.33% T4: 4.37% T5: 29.06% 
I1129 20:31:45.828342  1575 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 28.58% T1: 4.89% T2: 9.39% T3: 3.27% T4: 29.83% T5: 24.05% 
I1129 20:31:45.828351  1575 solver.cpp:223] Epoch 200 (iteration 15600), loss = 0.000560922
I1129 20:31:45.828372  1575 solver.cpp:240]     Train net output #0: loss = 0.000560922 (* 1 = 0.000560922 loss)
I1129 20:38:25.330749  1575 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 21.61% T1: 18.84% T2: 15.93% T3: 10.30% T4: 4.41% T5: 28.91% 
I1129 20:38:25.330879  1575 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 28.59% T1: 4.54% T2: 9.15% T3: 3.33% T4: 29.72% T5: 24.68% 
I1129 20:38:25.330889  1575 solver.cpp:223] Epoch 250 (iteration 19500), loss = 0.000906649
I1129 20:38:25.330912  1575 solver.cpp:240]     Train net output #0: loss = 0.000906649 (* 1 = 0.000906649 loss)
I1129 20:45:03.933838  1575 solver.cpp:284] Epoch 300, Testing net (#0)
I1129 20:45:07.868069  1575 solver.cpp:337]     Test net output #0: accuracy = 0.9723 (2.76999% error)
I1129 20:45:07.868152  1575 solver.cpp:341]     Test net output #1: loss = 0.136966 (* 1 = 0.136966 loss)
I1129 20:45:07.868162  1575 solver.cpp:355] Best score: 0.0276999 at Epoch 300
I1129 20:45:07.871536  1575 solver.cpp:394] Snapshotting solver state to snapshot/sicnn_train10k_mnist-sc_split4_best_97.23.solverstate
I1129 20:45:07.962867  1575 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 21.41% T1: 18.80% T2: 16.23% T3: 10.36% T4: 4.05% T5: 29.13% 
I1129 20:45:07.962898  1575 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 28.37% T1: 4.23% T2: 9.08% T3: 3.36% T4: 30.75% T5: 24.21% 
I1129 20:45:07.962916  1575 solver.cpp:223] Epoch 300 (iteration 23400), loss = 0.000290238
I1129 20:45:07.962936  1575 solver.cpp:240]     Train net output #0: loss = 0.000290238 (* 1 = 0.000290238 loss)
I1129 20:51:46.009001  1575 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 21.55% T1: 19.06% T2: 15.79% T3: 10.17% T4: 4.41% T5: 29.03% 
I1129 20:51:46.009099  1575 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 28.74% T1: 4.54% T2: 9.17% T3: 3.33% T4: 29.34% T5: 24.88% 
I1129 20:51:46.009106  1575 solver.cpp:223] Epoch 350 (iteration 27300), loss = 0.000968753
I1129 20:51:46.009145  1575 solver.cpp:240]     Train net output #0: loss = 0.000968753 (* 1 = 0.000968753 loss)
I1129 20:58:26.721830  1575 solver.cpp:386] Snapshotting to snapshot/sicnn_train10k_mnist-sc_split4_epoch_400.caffemodel
I1129 20:58:26.724205  1575 solver.cpp:394] Snapshotting solver state to snapshot/sicnn_train10k_mnist-sc_split4_epoch_400.solverstate
I1129 20:58:26.725292  1575 solver.cpp:284] Epoch 400, Testing net (#0)
I1129 20:58:30.677284  1575 solver.cpp:337]     Test net output #0: accuracy = 0.9677 (3.22999% error)
I1129 20:58:30.677381  1575 solver.cpp:341]     Test net output #1: loss = 0.162343 (* 1 = 0.162343 loss)
I1129 20:58:30.677393  1575 solver.cpp:355] Best score: 0.0322999 at Epoch 400
I1129 20:58:30.679421  1575 solver.cpp:394] Snapshotting solver state to snapshot/sicnn_train10k_mnist-sc_split4_best_96.77.solverstate
I1129 20:58:30.781370  1575 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 21.13% T1: 19.02% T2: 16.29% T3: 10.17% T4: 4.06% T5: 29.33% 
I1129 20:58:30.781417  1575 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 28.82% T1: 4.22% T2: 8.82% T3: 3.34% T4: 29.94% T5: 24.85% 
I1129 20:58:30.781425  1575 solver.cpp:223] Epoch 400 (iteration 31200), loss = 0.00126151
I1129 20:58:30.781446  1575 solver.cpp:240]     Train net output #0: loss = 0.00126151 (* 1 = 0.00126151 loss)
I1129 21:05:11.160838  1575 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 21.41% T1: 19.06% T2: 16.08% T3: 10.19% T4: 4.22% T5: 29.04% 
I1129 21:05:11.160940  1575 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 28.54% T1: 4.41% T2: 8.97% T3: 3.42% T4: 29.73% T5: 24.93% 
I1129 21:05:11.160949  1575 solver.cpp:223] Epoch 450 (iteration 35100), loss = 0.00100149
I1129 21:05:11.160974  1575 solver.cpp:240]     Train net output #0: loss = 0.00100149 (* 1 = 0.00100149 loss)
I1129 21:11:47.690387  1575 solver.cpp:284] Epoch 500, Testing net (#0)
I1129 21:11:51.608649  1575 solver.cpp:337]     Test net output #0: accuracy = 0.9752 (2.47998% error)
I1129 21:11:51.608737  1575 solver.cpp:341]     Test net output #1: loss = 0.138502 (* 1 = 0.138502 loss)
I1129 21:11:51.713814  1575 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 20.92% T1: 19.19% T2: 16.35% T3: 10.00% T4: 4.00% T5: 29.54% 
I1129 21:11:51.713862  1575 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 28.83% T1: 4.16% T2: 9.02% T3: 3.37% T4: 29.62% T5: 25.01% 
I1129 21:11:51.713871  1575 solver.cpp:223] Epoch 500 (iteration 39000), loss = 0.00101293
I1129 21:11:51.713891  1575 solver.cpp:240]     Train net output #0: loss = 0.00101293 (* 1 = 0.00101293 loss)
I1129 21:18:28.587082  1575 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 21.75% T1: 18.98% T2: 15.65% T3: 10.38% T4: 4.49% T5: 28.75% 
I1129 21:18:28.587201  1575 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 28.33% T1: 4.55% T2: 8.96% T3: 3.32% T4: 29.45% T5: 25.39% 
I1129 21:18:28.587210  1575 solver.cpp:223] Epoch 550 (iteration 42900), loss = 0.000644324
I1129 21:18:28.587249  1575 solver.cpp:240]     Train net output #0: loss = 0.000644324 (* 1 = 0.000644324 loss)
I1129 21:25:06.934700  1575 solver.cpp:386] Snapshotting to snapshot/sicnn_train10k_mnist-sc_split4_epoch_600.caffemodel
I1129 21:25:06.937036  1575 solver.cpp:394] Snapshotting solver state to snapshot/sicnn_train10k_mnist-sc_split4_epoch_600.solverstate
I1129 21:25:06.938032  1575 solver.cpp:284] Epoch 600, Testing net (#0)
I1129 21:25:10.809147  1575 solver.cpp:337]     Test net output #0: accuracy = 0.9741 (2.58999% error)
I1129 21:25:10.809244  1575 solver.cpp:341]     Test net output #1: loss = 0.133424 (* 1 = 0.133424 loss)
I1129 21:25:10.902834  1575 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 21.22% T1: 19.23% T2: 16.08% T3: 9.96% T4: 4.09% T5: 29.42% 
I1129 21:25:10.902878  1575 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 28.95% T1: 4.32% T2: 8.96% T3: 3.40% T4: 29.35% T5: 25.03% 
I1129 21:25:10.902886  1575 solver.cpp:223] Epoch 600 (iteration 46800), loss = 0.000649291
I1129 21:25:10.902905  1575 solver.cpp:240]     Train net output #0: loss = 0.000649291 (* 1 = 0.000649291 loss)
I1129 21:31:49.143100  1575 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 20.92% T1: 19.46% T2: 16.04% T3: 10.10% T4: 3.92% T5: 29.57% 
I1129 21:31:49.143200  1575 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 28.69% T1: 4.39% T2: 8.94% T3: 3.28% T4: 29.82% T5: 24.89% 
I1129 21:31:49.143209  1575 solver.cpp:223] Epoch 650 (iteration 50700), loss = 0.00124766
I1129 21:31:49.143232  1575 solver.cpp:240]     Train net output #0: loss = 0.00124766 (* 1 = 0.00124766 loss)
I1129 21:38:29.258441  1575 solver.cpp:284] Epoch 700, Testing net (#0)
I1129 21:38:33.127579  1575 solver.cpp:337]     Test net output #0: accuracy = 0.9737 (2.62997% error)
I1129 21:38:33.127674  1575 solver.cpp:341]     Test net output #1: loss = 0.11829 (* 1 = 0.11829 loss)
I1129 21:38:33.223927  1575 downpool_layer.cpp:369] conv1 DownPool, transformation usage: T0: 21.24% T1: 19.14% T2: 16.04% T3: 10.29% T4: 4.19% T5: 29.10% 
I1129 21:38:33.223980  1575 downpool_layer.cpp:369] conv2 DownPool, transformation usage: T0: 28.32% T1: 4.28% T2: 8.85% T3: 3.34% T4: 30.13% T5: 25.09% 
I1129 21:38:33.223989  1575 solver.cpp:223] Epoch 700 (iteration 54600), loss = 0.00118419
I1129 21:38:33.224011  1575 solver.cpp:240]     Train net output #0: loss = 0.00118419 (* 1 = 0.00118419 loss)
I1129 21:38:33.232178  1575 solver.cpp:386] Snapshotting to snapshot/sicnn_train10k_mnist-sc_split4_epoch_700.caffemodel
I1129 21:38:33.234527  1575 solver.cpp:394] Snapshotting solver state to snapshot/sicnn_train10k_mnist-sc_split4_epoch_700.solverstate
I1129 21:38:33.235520  1575 solver.cpp:270] Optimization Done.
I1129 21:38:33.235535  1575 caffe.cpp:124] Optimization Done.
