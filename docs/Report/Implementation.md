# SIE-CNNs Implementation (PyTorch)

All the implementation can be found in our [GitHub](https://github.com/wsgdrfz/SIE-CNN).
All the coding is done in PyTorch, and the code introduced in this documentation can be found under `src/pytorch/`.

## Code Sturcture
- `Make_datasets_Scale.py`: Generate scale dataset from the original one, for example, from MNIST to scale-MNIST.
- `Standard_CNN.py`: The baseline, a standard convolutional neural network with 3 convolution layers and 2 fully connected layers.
- `SI_ConvNet.py`: The implementation of [1].
    - `class ScaleInvariance_Layer`: The scale-invariance layer inherit from `nn.Module` including the `__init__` function and `forward` function.
    - `class Net_scaleinvariant_mnist_scale`: The network used for evaluation with same architecture as `Standard_CNN`: 3 convolution layers and 2 fully connected layers.
- `SS_CNN.py`: The implementation of [2].
    - `def generate_filter_basis`: Return a scale-steerable basis filter $S^{kj}$ for a specific pair of filter order $k$ and filter orientation $\phi_j$ based on the equation (1) from [2].
    - `class steerable_conv`: Return the scale-steered filter $W$, which is a linear combination of the proposed steerable basis. For different scale factor $s$, calculate the corresponding $W$ based on the equation (3) from [2].
    - `class ScaleConv_steering:` The scale-invariant layer with scale-steered filter inherit from `nn.Module` including the `__init__` function and `forward` function.
    - `class Net_steerinvariant_mnistlocal_scale`: The network used for evaluation with same structure as `SI_ConvNet`.
-  `Antialiased_SSCNN.py`:
    - `class Downsample`: Anti-aliasing common downsampling layer
    - `class Antialiased_SSCNN.py`: Antialiased version of `SS_CNN`.
- `main_test.py`: The main process of loading dataset, training networks and testing networks.

## Evaluation on MINIST-Scale

### MINIST-Scale Generation
The MNIST-Scale is generated by scaling the original MNIST dataset with a random scale factor $s \in (0.3,1)$. The MNIST-Scale dataset is randomized and then partitioned into 3 parts: 10k for training, 2k for validation and 50k for testing. The same operation is repeated for 6 times to create 6 splits.

### Standard CNN
For a fair comparison, all the networks have a total of 3 convolution layers and 2 fully connected layers. The number of trainable parameters for all the networks are kept approximately the same.

#### Network Architecture
The details of the network architecture are shown below:
```
convolution layer 1
max pooling layer 1: 2x2
batch normalization
convolution layer 2
max pooling layer 2: 2x2
batch normalization
convolution layer 3
max pooling layer 3: 8x8
batch normalization
fully connected layer 1
batch normalization
relu
dropout
fully connected layer 2
```

### SI-ConvNet
#### Network Architecture
The details of the network architecture are shown below:
```
scale-invariant convolution layer 1
max pooling layer 1: 2x2
batch normalization
scale-invariant convolution layer 2
max pooling layer 2: 2x2
batch normalization
scale-invariant convolution layer 3
max pooling layer 3: 8x8
batch normalization
fully connected layer 1
batch normalization
relu
dropout
fully connected layer 2
```
Each of the scale-invariant convolution layers has a structure shown in the figure below. We first scale the input to different size using scale factor $s \in [\frac{7}{11}, \frac{17}{11}]$, and convolve the same filter of size $11 \times 11$ with each scaled input. Then we scale the feature maps to a canonical size and max-pool the response over all the scales at each spatial location.
![Scale-invariant Convolution Layer](https://tva1.sinaimg.cn/large/006tNbRwgy1ga3t0k6cxij30oi0j2jxg.jpg)

### SS-CNN
#### Network Architecture
The details of the network architecture are shown below:
```
scale-invariant convolution layer with scale-steered filters layer 1
max pooling layer 1: 2x2
batch normalization
scale-invariant convolution layer with scale-steered filters layer 2
max pooling layer 2: 2x2
batch normalization
scale-invariant convolution layer with scale-steered filters layer 3
max pooling layer 3: 8x8
batch normalization
fully connected layer 1
batch normalization
relu
dropout
fully connected layer 2
```
Each of the convolution layers in the network uses the structure in the figure below. The scale-steered kernels are the linear combination of the scale-steerable basis. Then they scale the scale-steered kernels to different sizes to convolve with the input and output the max-pooling responses over scales.
![Scale-invariant layer](https://tva1.sinaimg.cn/large/006tNbRwgy1g9hu0pzy5rj30zu0hc0wo.jpg)

#### Parameters
For the scale-steerable basis filters, they keep the parameters as following:
$\beta=0, k=(0.5,1,2),\phi_j=j(\pi/8),j\in[1,8],\sigma_\phi=\pi/16$.
In this way, each scale-steerable basis filter has $3(\text{range of }k)\times 8(\text{range of }j)\times 2(\text{real and imaginary components})=48$ trainable parameters.

In the SS-CNN layer, each scale-steered kernel is a linear combination of the scale-steerable basis, and the kernel size ranges from $(7,7)$ to $(17,17)$ with only odd size chosen.

### Antialiased-SS-CNN
As described in [3], classic anti-aliasing can be integrated with existing downsampling to improve shift-equivariance of deep networks. Here, we combine anti-aliasing with SS-CNN. There are mainly three changes as shown in image below:
![](https://camo.githubusercontent.com/60d2cf12b269eafb56a597531c539a316116ac93/68747470733a2f2f726963687a68616e672e6769746875622e696f2f616e7469616c69617365642d636e6e732f7265736f75726365732f616e7469616c6961735f6d6f642e6a7067)
- `[nn.MaxPool2d(kernel_size=k, stride=s)]` to `[nn.MaxPool2d(kernel_size=k, stride=1), 
Downsample(channels=C, filt_size=M, stride=s)]`
- `[nn.Conv2d(Cin,C,kernel_size=k,stride=s,padding=1),
nn.ReLU(inplace=True)]` to `[nn.Conv2d(Cin,C,kernel_size=k,stride=1,padding=1),
nn.ReLU(inplace=True), 
Downsample(channels=C, filt_size=M, stride=s)]`
- `nn.AvgPool2d(kernel_size=k, stride=s)`to `Downsample(channels=C, filt_size=M, stride=s)`

### Result
All the experiments are performed on a GeForce RTX 2080 Ti.

- Training size 10k 
We use the training size of 10k and train for 300 epochs following the setup as in papers. The mean and standard deviations of the test errors over 6 splits are shown in the table below.

| Tran/Test Split | Standard CNN    | SI-ConvNet      | SS-CNN          | Antialiased-SS-CNN |
| --------------- | --------------- | --------------- | --------------- | ------------------ |
| 0               | 3.06            | 1.92            | 1.74            | 1.66               |
| 1               | 3.16            | 1.93            | 1.97            | 1.90               |
| 2               | 3.27            | 2.05            | 1.88            | 1.87               |
| 3               | 3.22            | 2.02            | 2.18            | 2.12               |
| 4               | 3.34            | 1.97            | 2.02            | 1.86               |
| 5               | 3.52            | 2.07            | 2.12            | 2.01               |
| Range           | 3.26 $\pm$ 0.15 | 2.00 $\pm$ 0.06 | 1.98 $\pm$ 0.15 | 1.90 $\pm$ 0.14    |




- Training size 1k 
To evaluate the generalization ability of the networks, we also use the training size of 1k and train for 300 epochs. The results are shown in the table below. 

| Tran/Test Split | Standard CNN    | SI-ConvNet      | SS-CNN          | Antialiased-SS-CNN |
| --------------- | --------------- | --------------- | --------------- | ------------------ |
| 0               | 7.59            | 4.66            | 4.31            | 4.30               |
| 1               | 7.32            | 4.60            | 4.13            | 4.36               |
| 2               | 7.01            | 4.50            | 3.90            | 4.59               |
| 3               | 8.42            | 4.23            | 4.11            | 4.78               |
| 4               | 7.79            | 4.93            | 4.20            | 5.31               |
| 5               | 7.73            | 4.43            | 3.99            | 5.35               |
| Range           | 7.64 $\pm$ 0.44 | 4.56 $\pm$ 0.21 | 4.11 $\pm$ 0.13 | 4.78 $\pm$ 0.42    |

## Reference
[1] Angjoo K, Abhishek S, David J. Locally Scale-Invariant Convolutional Neural Networks[J]. arXiv preprint arXiv: 1412.5104, 2014.
[2] Ghosh R, Gupta A K. Scale Steerable Filters for Locally Scale-Invariant Convolutional NeuralNetworks[J]. arXiv preprint arXiv:1906.03861, 2019. 
[3] Zhang, Richard. "Making convolutional networks shift-invariant again." arXiv preprint arXiv:1904.11486 (2019).
APA	